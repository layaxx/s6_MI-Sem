\documentclass[../main.tex]{subfiles}
\begin{document}

With the recent rise in popularity of large language models such as GPT3\cite{openai_2021} or DALL-E\cite{dalle-https://doi.org/10.48550/arxiv.2204.06125},
and companies beginning to bridge the gap between research and commercial use,
it is important to keep in mind that such models may not be free of biases.\cite{gpt3-https://doi.org/10.48550/arxiv.2102.02503}

This paper looks at approaches to quantify bias in both actual systems and underlying corpora of multi-document summarization (MDS) models.
MDS has use cases such as review or news aggregation, and as such biases can have noticeable impact.\cite{dey-etal-2020-corpora}

Starting from a 2020 paper\cite{dey-etal-2020-corpora} dealing with this topic,
the metrics for quantifying biases proposed in this paper as well as its impact and possible alternative criteria will be discussed.
Finally, the metrics will be applied to a recently published corpus, for which they where not provided at publication and possible conclusions about its structure will be discussed.

\end{document}