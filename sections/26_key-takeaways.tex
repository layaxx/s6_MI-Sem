\documentclass[20_original-paper.tex]{subfiles}
\begin{document}

The authors state that a lack of clear definition for MDS tasks leads to a lack of a single (or a few) standardized dataset.
Instead, most scientists provide their own custom dataset with newly proposed MDS systems.
This severely reduces the comparability between systems for two reasons.

For one, the authors have established the influence a specific dataset can have on the performance of MDS system (see \ref{sec:results}).
Therefore, the reported performance of a newly proposed system could be inflated when it is only evaluated on a custom corpus as compared to established corpora.
However, using an existing corpus may not always be viable, due to the unstandardized nature of MDS tasks.
To increase comparability between datasets, the authors thus aim to establish metrics for comparing datasets.
In order to be useful, these metrics should be reported by researchers creating a new corpus.
Objective metrics include Pyramid and Inverse Pyramid Scores and are assigned a higher importance by the authors, who state those two "must be
reported as they are strong indicators of generic corpus quality"\cite{dey-etal-2020-corpora}.


Regarding biases, the original paper focuses mostly on structural bias as opposed to in the content.
Due to layout bias, a system trained on a news corpus may now perform well on other corpora with documents following a similar structure, but also significantly worse on collections of documents that either have no layout bias at all or a different, not as front-loaded, kind of layout bias.\cite{leverage-lead-bias-https://doi.org/10.48550/arxiv.1912.11602}
The authors establish a connection between bias in training corpus and system predictions, but point out the need for future research regarding explicit causes and potential counter measures.

\end{document}