% SAMPLE REFERENCE
@inproceedings{dey-etal-2020-corpora,
  title     = {Corpora Evaluation and System Bias Detection in Multi-document Summarization},
  author    = {Dey, Alvin  and
               Chowdhury, Tanya  and
               Kumar, Yash  and
               Chakraborty, Tanmoy},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.findings-emnlp.254},
  doi       = {10.18653/v1/2020.findings-emnlp.254},
  pages     = {2830--2840},
  abstract  = {Multi-document summarization (MDS) is the task of reflecting key points from any set of documents into a concise text paragraph. In the past, it has been used to aggregate news, tweets, product reviews, etc. from various sources. Owing to no standard definition of the task, we encounter a plethora of datasets with varying levels of overlap and conflict between participating documents. There is also no standard regarding what constitutes summary information in MDS. Adding to the challenge is the fact that new systems report results on a set of chosen datasets, which might not correlate with their performance on the other datasets. In this paper, we study this heterogeneous task with the help of a few widely used MDS corpora and a suite of state-of-theart models. We make an attempt to quantify the quality of summarization corpus and prescribe a list of points to consider while proposing a new MDS corpus. Next, we analyze the reason behind the absence of an MDS system which achieves superior performance across all corpora. We then observe the extent to which system metrics are influenced, and bias is propagated due to corpus properties. The scripts to reproduce the experiments in this work are available at https://github.com/LCS2-IIITD/summarization{\_}bias.git}
}
@inproceedings{Valenzuela2015IdentifyingMC,
  title     = {Identifying Meaningful Citations},
  author    = {Marco Valenzuela and Vu A. Ha and Oren Etzioni},
  booktitle = {AAAI Workshop: Scholarly Big Data},
  year      = {2015}
}
@inproceedings{Cohan2019StructuralSF,
  title     = {Structural Scaffolds for Citation Intent Classification in Scientific Publications},
  author    = {Arman Cohan and Waleed Ammar and Madeleine van Zuylen and Field Cady},
  booktitle = {NAACL},
  year      = {2019}
}
@inproceedings{Fabbri2021ConvoSummCS,
  title     = {ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining},
  author    = {Alexander R. Fabbri and Faiaz Rahman and Imad Rizvi and Borui Wang and Haoran Li and Yashar Mehdad and Dragomir Radev},
  booktitle = {ACL},
  year      = {2021}
}
@inproceedings{lin-2004-rouge,
  title     = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
  author    = {Lin, Chin-Yew},
  booktitle = {Text Summarization Branches Out},
  month     = jul,
  year      = {2004},
  address   = {Barcelona, Spain},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W04-1013},
  pages     = {74--81}
}
@article{Chen2022TwophaseME,
  title   = {Two-phase Multi-document Event Summarization on Core Event Graphs},
  author  = {Zengjian Chen and Jin Xu and M. Liao and Tong Xue and Kun He},
  journal = {J. Artif. Intell. Res.},
  year    = {2022},
  volume  = {74},
  pages   = {1037-1057}
}
‌@inproceedings{Moro2022SemanticSF,
  title     = {Semantic Self-Segmentation for Abstractive Summarization of Long Documents in Low-Resource Regimes},
  author    = {Gianluca Moro and Luca Ragazzi},
  booktitle = {AAAI},
  year      = {2022}
}
@article{Adams2021WhatsIA,
  title   = {What’s in a Summary? Laying the Groundwork for Advances in Hospital-Course Summarization},
  author  = {Griffin Adams and Emily Alsentzer and Mert Ketenci and Jason E Zucker and No{\'e}mie Elhadad},
  journal = {Proceedings of the conference. Association for Computational Linguistics. North American Chapter. Meeting},
  year    = {2021},
  volume  = {2021},
  pages   = {4794-4811}
}
@inproceedings{Counseling,
  author    = {Srivastava, Aseem and Suresh, Tharun and Lord, Sarah P. and Akhtar, Md Shad and Chakraborty, Tanmoy},
  title     = {Counseling Summarization Using Mental Health Knowledge Guided Utterance Filtering},
  year      = {2022},
  isbn      = {9781450393850},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3534678.3539187},
  doi       = {10.1145/3534678.3539187},
  abstract  = {The psychotherapy intervention technique is a multifaceted conversation between a therapist and a patient. Unlike general clinical discussions, psychotherapy's core components (viz. symptoms) are hard to distinguish, thus becoming a complex problem to summarize later. A structured counseling conversation may contain discussions about symptoms, history of mental health issues, or the discovery of the patient's behavior. It may also contain discussion filler words irrelevant to a clinical summary. We refer to these elements of structured psychotherapy as counseling components. In this paper, the aim is mental health counseling summarization to build upon domain knowledge and to help clinicians quickly glean meaning. We create a new dataset after annotating 12.9K utterances of counseling components and reference summaries for each dialogue. Further, we propose ConSum, a novel counseling-component guided summarization model. ConSum undergoes three independent modules. First, to assess the presence of depressive symptoms, it filters utterances utilizing the Patient Health Questionnaire (PHQ-9), while the second and third modules aim to classify counseling components. At last, we propose a problem-specific Mental Health Information Capture (MHIC) evaluation metric for counseling summaries. Our comparative study shows that we improve on performance and generate cohesive, semantic, and coherent summaries. We comprehensively analyze the generated summaries to investigate the capturing of psychotherapy elements. Human and clinical evaluations on the summary show that ConSum generates quality summary. Further, mental health experts validate the clinical acceptability of the ConSum. Lastly, we discuss the uniqueness in mental health counseling summarization in the real world and show evidences of its deployment on an online application with the support of mpathic.ai},
  booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages     = {3920–3930},
  numpages  = {11},
  keywords  = {dialogue summarization, natural language processing},
  location  = {Washington DC, USA},
  series    = {KDD '22}
}
@inproceedings{WCEP-gholipour-ghalandari-etal-2020-large,
  title     = {A Large-Scale Multi-Document Summarization Dataset from the {W}ikipedia Current Events Portal},
  author    = {Gholipour Ghalandari, Demian  and
               Hokamp, Chris  and
               Pham, Nghia The  and
               Glover, John  and
               Ifrim, Georgiana},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.120},
  doi       = {10.18653/v1/2020.acl-main.120},
  pages     = {1302--1308},
  abstract  = {Multi-document summarization (MDS) aims to compress the content in large document collections into short summaries and has important applications in story clustering for newsfeeds, presentation of search results, and timeline generation. However, there is a lack of datasets that realistically address such use cases at a scale large enough for training supervised models for this task. This work presents a new dataset for MDS that is large both in the total number of document clusters and in the size of individual clusters. We build this dataset by leveraging the Wikipedia Current Events Portal (WCEP), which provides concise and neutral human-written summaries of news events, with links to external source articles. We also automatically extend these source articles by looking for related articles in the Common Crawl archive. We provide a quantitative analysis of the dataset and empirical results for several state-of-the-art MDS techniques.}
}
@misc{WCEP-Github-complementizer_2022,
  title   = {complementizer/wcep-mds-dataset},
  url     = {https://github.com/complementizer/wcep-mds-dataset},
  journal = {GitHub},
  author  = {complementizer},
  year    = {2022},
  month   = {Feb}
}
@article{zopf_maxime_peyrard_eckle-kohler_2016,
  title   = {The Next Step for Multi-Document Summarization: A Heterogeneous Multi-Genre Corpus Built with a Novel Construction Approach},
  url     = {https://aclanthology.org/C16-1145/},
  journal = {ACL Anthology},
  author  = {Zopf, Markus and Maxime Peyrard and Eckle-Kohler, Judith},
  year    = {2016},
  month   = {Dec},
  pages   = {1535–1545}
}
@article{Exposure_bias_wang_sennrich_2020,
  title   = {On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation},
  url     = {https://aclanthology.org/2020.acl-main.326/},
  doi     = {10.18653/v1/2020.acl-main.326},
  journal = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author  = {Wang, Chaojun and Sennrich, Rico},
  year    = {2020}
}
@inproceedings{nadeem-etal-2021-stereoset,
  title     = {{S}tereo{S}et: Measuring stereotypical bias in pretrained language models},
  author    = {Nadeem, Moin  and
               Bethke, Anna  and
               Reddy, Siva},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.acl-long.416},
  doi       = {10.18653/v1/2021.acl-long.416},
  pages     = {5356--5371},
  abstract  = {A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.}
}
‌
‌
‌